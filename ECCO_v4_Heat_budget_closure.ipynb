{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Heat Budget Closure\n",
    "*Contributors*: [Jan-Erik Tesdal](https://github.com/jetesdal), [Ryan Abernathey](https://github.com/rabernat), [Ian Fenty](https://github.com/ifenty), and [Andrew Delman](https://github.com/andrewdelman)\n",
    "\n",
    "Updated 2024-10-17\n",
    "\n",
    "A major part of this tutorial is based on \"*A Note on Practical Evaluation of Budgets in ECCO Version 4 Release 3\"* by Christopher G. Piecuch (https://ecco.jpl.nasa.gov/drive/files/Version4/Release3/doc/v4r3_budgets_howto.pdf). Calculation steps and Python code presented here are converted from the MATLAB code presented in the above reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Evaluating and closing the heat budget over the global ocean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The ocean heat content (OHC) variability is described here with potential temperature ($\\theta$) which is given by the ECCOv4 diagnostic output `THETA`. The budget equation describing the change in $\\theta$ is evaluated in general as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\theta}{\\partial t} = -\\nabla \\cdot (\\theta \\mathbf{u})-\\nabla\\cdot\\mathbf{F}_\\textrm{diff}^{\\theta}+{F}_\\textrm{forc}^{\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heat budget includes the change in temperature over time ($\\frac{\\partial \\theta}{\\partial t}$), the convergence of heat advection ($-\\nabla \\cdot (\\theta \\mathbf{u})$) and heat diffusion ($-\\nabla\\cdot\\mathbf{F}_\\textrm{diff}$), plus downward heat flux from the atmosphere (${F}_\\textrm{forc}$). Note that in our definition ${F}_\\textrm{forc}$ contains both latent and sensible air-sea heat fluxes, longwave and shortwave radiation, as well as geothermal heat flux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the special case of ECCOv4, the heat budget is formulated as\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial(s^*\\theta)}{\\partial t}}_{G^{\\theta}_\\textrm{total}} = \\underbrace{-\\nabla_{z^{*}} \\cdot(s^*\\theta\\,\\mathbf{v}_{res}) - \\frac{\\partial(\\theta\\,w_{res})}{\\partial z^{*}}}_{G^{\\theta}_\\textrm{advection}}\\underbrace{- s^* ({\\nabla\\cdot\\mathbf{F}_\\textrm{diff}^{\\theta}})}_{G^{\\theta}_\\textrm{diffusion}} + \\underbrace{s^* {F}_\\textrm{forc}^{\\theta}}_{G^{\\theta}_\\textrm{forcing}}\n",
    "$$\n",
    "\n",
    "where $z^{*} = \\frac{z - \\eta}{H + \\eta}H$ and $\\nabla_{z^{*}}$/$\\frac{\\partial}{\\partial z^{*}}$ are horizontal/vertical divergences in the $z^*$ frame. Also note that the advection is now separated into horizontal ($\\mathbf{v}_{res}$) and vertical ($w_{res}$) components, and there is a scaling factor ($s^* = 1+ \\frac{\\eta}{H}$) applied to the horizontal advection as well as the diffusion term ($G^{\\theta}_\\textrm{diffusion}$) and forcing term ($G^{\\theta}_\\textrm{forcing}$). $s^*$ is a function of $\\eta$ which is the displacement of the ocean surface from its resting position of $z=0$ (i.e., sea height anomaly). $H$ is the ocean depth. $s^{*}$ comes from the coordinate transformation from z to $z^*$ (Campin and Adcroft, 2004; Campin et al., 2004). See [ECCOv4 Global Volume Budget Closure](https://ecco-v4-python-tutorial.readthedocs.io/ECCO_v4_Volume_budget_closure.html#ECCOv4-Global-Volume-Budget-Closure) for a more detailed explanation of the $z^*$ coordinate system.\n",
    "\n",
    "Note that the velocity terms in the ECCOv4 heat budget equation ($\\mathbf{v}_{res}$ and $w_{res}$) are described as the \"residual mean\" velocities, which contain both the resolved (Eulerian) flow field, as well as the \"GM bolus\" velocity (i.e., parameterizing unresolved eddy effects):\n",
    "\n",
    "$$(u_{res},v_{res},w_{res})= (u,v,w)+ (u_b,v_b,w_b)$$\n",
    "\n",
    "Here $(u_b,v_b,w_b)$ is the bolus velocity parameter, taking into account the correlation between velocity and thickness (also known as the eddy induced transportor the eddy advection term)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the heat budget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evalute each term in the above heat budget \n",
    "\n",
    "$$G^{\\theta}_\\textrm{total} = G^{\\theta}_\\textrm{advection} + G^{\\theta}_\\textrm{diffusion} + G^{\\theta}_\\textrm{forcing}$$\n",
    "\n",
    "The total tendency of $\\theta$ ($G^{\\theta}_\\textrm{total}$) is the sum of the $\\theta$ tendencies from advective heat convergence ($G^{\\theta}_\\textrm{advection}$), diffusive heat convergence ($G^{\\theta}_\\textrm{diffusion}$) and total forcing ($G^{\\theta}_\\textrm{forcing}$). \n",
    "\n",
    "We present calculation sequentially for each term starting with $G^{\\theta}_\\textrm{total}$ which will be derived by differencing instantaneous monthly snapshots of $\\theta$. The terms on the right hand side of the heat budget are derived from monthly-averaged fields.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "Here are the ShortNames of the NASA Earthdata datasets that are needed for this tutorial:\n",
    "\n",
    "- **ECCO_L4_GEOMETRY_LLC0090GRID_V4R4**\n",
    "- **ECCO_L4_OCEAN_3D_TEMPERATURE_FLUX_LLC0090GRID_MONTHLY_V4R4** (1993-2016)\n",
    "- **ECCO_L4_HEAT_FLUX_LLC0090GRID_MONTHLY_V4R4** (1993-2016)\n",
    "- **ECCO_L4_SSH_LLC0090GRID_SNAPSHOT_V4R4** (1993/1/1-2017/1/1, 1st of each month)\n",
    "- **ECCO_L4_TEMP_SALINITY_LLC0090GRID_SNAPSHOT_V4R4** (1993/1/1-2017/1/1, 1st of each month)\n",
    "\n",
    "If you haven't yet [set up](https://ecco-v4-python-tutorial.readthedocs.io/ECCO_access_intro.html#Setting-up-ecco_access) the `ecco_access` package in your path, you should do that before running this notebook. The `ecco_access.ecco_podaac_to_xrdataset` function used in the notebooks will handle the downloads or (in the AWS Cloud) direct access of the output, and open the data as an `xarray` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment and load ECCOv4 diagnostic output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "\n",
    "from os.path import join,expanduser,exists,split\n",
    "user_home_dir = expanduser('~')\n",
    "\n",
    "sys.path.insert(0,join(user_home_dir))\n",
    "import ecco_access as ea\n",
    "\n",
    "# indicate mode of access\n",
    "# options are:\n",
    "# 'download': direct download from internet to your local machine\n",
    "# 'download_ifspace': like download, but only proceeds \n",
    "#                     if your machine have sufficient storage\n",
    "# 's3_open': access datasets in-cloud from an AWS instance\n",
    "# 's3_open_fsspec': use jsons generated with fsspec and \n",
    "#                   kerchunk libraries to speed up in-cloud access\n",
    "# 's3_get': direct download from S3 in-cloud to an AWS instance\n",
    "# 's3_get_ifspace': like s3_get, but only proceeds if your instance \n",
    "#                   has sufficient storage\n",
    "access_mode = 'query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning messages for a cleaner presentation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import psutil\n",
    "\n",
    "# # setting up a dask LocalCluster (only if number cores available >= 4 and available memory/core >= 2 GB)\n",
    "# distributed_cores_min = 4\n",
    "# distributed_mem_per_core_min = 2*(10**9)\n",
    "# mem_per_core = psutil.virtual_memory().available/os.cpu_count()\n",
    "# if ((os.cpu_count() >= distributed_cores_min) and \\\n",
    "#   (mem_per_core >= distributed_mem_per_core_min)):\n",
    "#     from dask.distributed import Client\n",
    "#     from dask.distributed import LocalCluster\n",
    "#     cluster = LocalCluster()\n",
    "#     client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "#  connec to existing LocalCluster\n",
    "# the port number will be different!\n",
    "client = Client(\"tcp://127.0.0.1:35497\")\n",
    "client.ncores\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add relevant constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seawater density (kg/m^3)\n",
    "rhoconst = 1029\n",
    "## needed to convert surface mass fluxes to volume fluxes\n",
    "\n",
    "# Heat capacity (J/kg/K)\n",
    "c_p = 3994\n",
    "\n",
    "# Constants for surface heat penetration (from Table 2 of Paulson and Simpson, 1977)\n",
    "R = 0.62 \n",
    "zeta1 = 0.6\n",
    "zeta2 = 20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ecco_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set top-level file directory for the ECCO NetCDF files\n",
    "## =================================================================\n",
    "\n",
    "## currently set to ~/Downloads/ECCO_V4r4_PODAAC\n",
    "ECCO_dir = join(user_home_dir,'Downloads','ECCO_V4r4_PODAAC')\n",
    "\n",
    "# # for access_mode = 's3_open_fsspec', need to specify the root directory \n",
    "# # containing the jsons\n",
    "# jsons_root_dir = join('/efs_ecco','mzz-jsons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'feed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m StartDate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1993-01\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m EndDate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2016-12\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m ds_dict \u001b[38;5;241m=\u001b[39m \u001b[43mea\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mecco_podaac_to_xrdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mShortNames_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mStartDate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStartDate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mEndDate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEndDate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmonthly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms3_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;66;43;03m# download_root_dir=ECCO_dir,\\\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmax_avail_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ecco_access/ecco_access.py:411\u001b[0m, in \u001b[0;36mecco_podaac_to_xrdataset\u001b[0;34m(query, version, grid, time_res, StartDate, EndDate, snapshot_interval, mode, download_root_dir, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# submit access query (and download if needed)\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m access_output \u001b[38;5;241m=\u001b[39m \u001b[43mecco_podaac_access\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtime_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mStartDate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mEndDate\u001b[49m\u001b[43m,\u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdownload_root_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# determine value of snapshot_interval if None or not specified\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m snapshot_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/ecco_access/ecco_access.py:179\u001b[0m, in \u001b[0;36mecco_podaac_access\u001b[0;34m(query, version, grid, time_res, StartDate, EndDate, snapshot_interval, mode, download_root_dir, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m         shortnames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m shortnames_find(curr_query,\\\n\u001b[1;32m    176\u001b[0m                                       grid\u001b[38;5;241m=\u001b[39mcurr_grid,\\\n\u001b[1;32m    177\u001b[0m                                       time_res\u001b[38;5;241m=\u001b[39mcurr_time_res)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     shortnames \u001b[38;5;241m=\u001b[39m \u001b[43mshortnames_find\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtime_res\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m## query NASA Earthdata CMR and download granules\u001b[39;00m\n\u001b[1;32m    184\u001b[0m possible_mode_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mls\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3_ls\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3_query\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    185\u001b[0m                      \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload_ifspace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload_subset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    186\u001b[0m                      \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3_open\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3_get\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3_get_ifspace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3_open_fsspec\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/ecco_access/ecco_access.py:154\u001b[0m, in \u001b[0;36mecco_podaac_access.<locals>.shortnames_find\u001b[0;34m(query_list, grid, time_res)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query_item \u001b[38;5;129;01min\u001b[39;00m query_list:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# see if the query is an existing NASA Earthdata ShortName\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# if not, then do a text search of the ECCO variable lists\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://cmr.earthdata.nasa.gov/search/collections.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    153\u001b[0m                             params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShortName\u001b[39m\u001b[38;5;124m'\u001b[39m:query_item})\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentry\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    155\u001b[0m         shortnames_list\u001b[38;5;241m.\u001b[39mappend(query_item)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'feed'"
     ]
    }
   ],
   "source": [
    "## access datasets needed for this tutorial\n",
    "\n",
    "ShortNames_list = [\"ECCO_L4_GEOMETRY_LLC0090GRID_V4R4\",\\\n",
    "                   \"ECCO_L4_OCEAN_3D_TEMPERATURE_FLUX_LLC0090GRID_MONTHLY_V4R4\",\\\n",
    "                   \"ECCO_L4_HEAT_FLUX_LLC0090GRID_MONTHLY_V4R4\",\\\n",
    "                   \"ECCO_L4_SSH_LLC0090GRID_SNAPSHOT_V4R4\",\\\n",
    "                   \"ECCO_L4_TEMP_SALINITY_LLC0090GRID_SNAPSHOT_V4R4\"]\n",
    "StartDate = '1993-01'\n",
    "EndDate = '2016-12'\n",
    "ds_dict = ea.ecco_podaac_to_xrdataset(ShortNames_list,\\\n",
    "                                            StartDate=StartDate,EndDate=EndDate,\\\n",
    "                                            snapshot_interval='monthly',\\\n",
    "                                            mode='s3_open',\\\n",
    "                                            # download_root_dir=ECCO_dir,\\\n",
    "                                            max_avail_frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the ecco_v4_py library into Python\n",
    "## =========================================\n",
    "##    If ecco_v4_py is not installed in your local Python library, \n",
    "##    tell Python where to find it.  The example below adds\n",
    "##    ecco_v4_py to the user's path if it is stored in the folder\n",
    "##    ECCOv4-py under the user's home directory\n",
    "\n",
    "sys.path.append(join(user_home_dir,'ECCOv4-py'))\n",
    "\n",
    "import ecco_v4_py as ecco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load the model grid\n",
    "ecco_grid = ds_dict[ShortNames_list[0]].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume\n",
    "Calculate the volume of each grid cell. This is used when converting advective and diffusive flux convergences and calculating volume-weighted averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume (m^3)\n",
    "vol = (ecco_grid.rA*ecco_grid.drF*ecco_grid.hFacC).transpose('tile','k','j','i').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load monthly snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_start = 1993\n",
    "year_end = 2016\n",
    "\n",
    "# open ETAN and THETA snapshots (beginning of each month)\n",
    "ecco_monthly_SSH = ds_dict[ShortNames_list[3]]\n",
    "ecco_monthly_TS = ds_dict[ShortNames_list[4]]\n",
    "ecco_monthly_snaps = xr.merge((ecco_monthly_SSH['ETAN'],ecco_monthly_TS['THETA']))\n",
    "\n",
    "# time mask for snapshots\n",
    "time_snap_mask = np.logical_and(ecco_monthly_snaps.time.values >= np.datetime64(str(year_start)+'-01-01','ns'),\\\n",
    "                                ecco_monthly_snaps.time.values < np.datetime64(str(year_end+1)+'-01-02','ns'))\n",
    "\n",
    "ecco_monthly_snaps = ecco_monthly_snaps.isel(time=time_snap_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1993-01 (beginning of first month) to 2017-01-01 (end of last month, 2016-12)\n",
    "print(ecco_monthly_snaps.ETAN.time.isel(time=[0, -1]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the record of the last snapshot\n",
    "## This is used to defined the exact period for monthly mean data \n",
    "last_record_date = ecco.extract_yyyy_mm_dd_hh_mm_ss_from_datetime64(ecco_monthly_snaps.time[-1].values)\n",
    "print(last_record_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load monthly mean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Open ECCO monthly mean variables\n",
    "\n",
    "ecco_vars_int = ds_dict[ShortNames_list[1]]\n",
    "ecco_vars_sfc = ds_dict[ShortNames_list[2]]\n",
    "\n",
    "ecco_monthly_mean = xr.merge((ecco_vars_int,\\\n",
    "                              ecco_vars_sfc[['TFLUX','oceQsw']]))\n",
    "\n",
    "# time mask for monthly means\n",
    "time_mean_mask = np.logical_and(ecco_monthly_mean.time.values >= np.datetime64(str(year_start)+'-01-01','ns'),\\\n",
    "                                ecco_monthly_mean.time.values < np.datetime64(str(year_end+1)+'-01-01','ns'))\n",
    "\n",
    "ecco_monthly_mean = ecco_monthly_mean.isel(time=time_mean_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first and last time points of the monthly-mean records\n",
    "print(ecco_monthly_mean.time.isel(time=[0, -1]).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each monthly mean record is bookended by a snapshot. We should have one more snapshot than monthly mean record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of monthly mean records: ', len(ecco_monthly_mean.time))\n",
    "print('Number of monthly snapshot records: ', len(ecco_monthly_snaps.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop superfluous coordinates (We already have them in ecco_grid)\n",
    "ecco_monthly_mean = ecco_monthly_mean.reset_coords(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataset of monthly mean and snapshots data\n",
    "Merge the two datasets to put everything into one single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.merge([ecco_monthly_mean,\n",
    "               ecco_monthly_snaps.rename({'time':'time_snp','ETAN':'ETAN_snp', 'THETA':'THETA_snp'})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the xgcm 'grid' object\n",
    "\n",
    "The `xgcm` 'grid' object is used to calculate the flux divergences across different tiles of the lat-lon-cap grid and the time derivatives from ``THETA`` snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change time axis of the snapshot variables\n",
    "ds.time_snp.attrs['c_grid_axis_shift'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = ecco.get_llc_grid(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of seconds in each month\n",
    "The xgcm `grid` object includes information on the time axis, such that we can use it to get $\\Delta t$, which is the time span between the beginning and end of each month (in seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = grid.diff(ds.time_snp, 'T', boundary='fill', fill_value=np.nan)\n",
    "\n",
    "# Convert to seconds\n",
    "delta_t = delta_t.astype('f4') / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate total tendency of $\\theta$ ($G^{\\theta}_\\textrm{total}$)\n",
    "\n",
    "We calculate the monthly-averaged time tendency of ``THETA`` by differencing monthly ``THETA`` snapshots. Remember  that we need to include a scaling factor due to the nonlinear free surface formulation. Thus, we need to use snapshots of both `ETAN` and `THETA`  to evaluate $s^*\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the s*theta term\n",
    "sTHETA = ds.THETA_snp*(1+ds.ETAN_snp/ecco_grid.Depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total tendency (psu/s)\n",
    "G_total = sTHETA.diff(dim='time_snp')/np.expand_dims(delta_t.values,axis=(1,2,3,4))\n",
    "\n",
    "# re-assign and rename time coordinate\n",
    "G_total = G_total.rename({'time_snp':'time'})\n",
    "G_total = G_total.assign_coords({'time':delta_t.time.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Unlike the monthly snapshots `ETAN_snp` and `THETA_snp`, the resulting data array `G_total` has now the same time values as the time-mean fields (middle of the month)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the time-mean $\\partial \\theta / \\partial t$, total $\\Delta \\theta$, and one example $\\partial \\theta / \\partial t$ field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Time-mean $\\partial \\theta / \\partial t$\n",
    "The time-mean $\\partial \\theta / \\partial t$ (i.e., $\\overline{G^{\\theta}_\\textrm{total}}$), is given by \n",
    "\n",
    "$\\overline{G^{\\theta}_\\textrm{total}} = \\sum_{i=1}^{nm} w_i G^{\\theta}_\\textrm{total}$\n",
    "\n",
    "with $\\sum_{i=1}^{nm} w_i = 1$ and  nm=number of months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The weights are just the number of seconds per month divided by total seconds\n",
    "month_length_weights = delta_t / delta_t.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weighted mean weights by the length of each month (in seconds)\n",
    "G_total_mean = (G_total*month_length_weights).sum('time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "for idx, k in enumerate([0,10,25]):\n",
    "    p = ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC, G_total_mean.isel(k=k),show_colorbar=True,\n",
    "                                      cmap='RdBu_r', user_lon_0=-67, dx=2, dy=2, subplot_grid=[3,1,idx+1]);\n",
    "    p[1].set_title(r'$\\overline{G^\\theta_{total}}$ at z = %i m (k = %i) [$^\\circ$C s$^{-1}$]'\\\n",
    "                   %(np.round(-ecco_grid.Z[k].values),k), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total $\\Delta \\theta$\n",
    "\n",
    "How much did ``THETA`` change over the analysis period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of seconds in the entire period \n",
    "seconds_in_entire_period = \\\n",
    "    float(ds.time_snp[-1] - ds.time_snp[0])/1e9\n",
    "print ('seconds in analysis period: ', seconds_in_entire_period)\n",
    "\n",
    "# which is also the sum of the number of seconds in each month\n",
    "print('Sum of seconds in each month ', delta_t.sum().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THETA_delta = G_total_mean*seconds_in_entire_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5));\n",
    "ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC, \\\n",
    "                              THETA_delta.isel(k=0),show_colorbar=True,\\\n",
    "                              cmin=-4, cmax=4, \\\n",
    "                              cmap='RdBu_r', user_lon_0=-67, dx=0.2, dy=0.2);\n",
    "plt.title(r'Predicted $\\Delta \\theta$ at the sea surface [$^\\circ$C] from $\\overline{G^\\theta_{total}}$',fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sanity check the total ``THETA`` change that we found by multipling the time-mean ``THETA`` tendency with the number of seconds in the simulation by comparing that with the difference in ``THETA`` between the end of the last month and start of the first month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THETA_delta_method_2 = ds.THETA_snp.isel(time_snp=-1) - ds.THETA_snp.isel(time_snp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5));\n",
    "ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC, \\\n",
    "                              THETA_delta_method_2.isel(k=0),show_colorbar=True,\\\n",
    "                              cmin=-4, cmax=4, \\\n",
    "                              cmap='RdBu_r', user_lon_0=-67, dx=0.2, dy=0.2);\n",
    "plt.title(r'Actual $\\Delta \\theta$ [$^\\circ$C]', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example $G^\\theta_{total}$ field at a particular time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an array of YYYY, MM, DD, HH, MM, SS for \n",
    "#dETAN_dT_perSec at time index 100\n",
    "curr_t_ind = 100\n",
    "tmp = str(G_total.time.values[curr_t_ind])\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5));\n",
    "ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC, G_total.isel(time=curr_t_ind,k=0), show_colorbar=True,\n",
    "                              cmap='RdBu_r', user_lon_0=-67, dx=0.2, dy=0.2);\n",
    "\n",
    "plt.title(r'$G^\\theta_{total}$ at the sea surface [$^\\circ$C s$^{-1}$] during ' + \n",
    "          str(tmp)[0:4] +'/' + str(tmp)[5:7], fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any given month the time rate of change of ``THETA`` is strongly dependent on the season. In the above we are looking at May 2001. We see positive ``THETA`` tendency in the northern hemisphere and cooling in the southern hemisphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate tendency due to advective convergence ($G^{\\theta}_\\textrm{advection}$)\n",
    "\n",
    "The relevant fields from the diagnostic output here are\n",
    "- `ADVx_TH`: U Component Advective Flux of Potential Temperature (degC m^3/s)\n",
    "- `ADVy_TH`: V Component Advective Flux of Potential Temperature (degC m^3/s)\n",
    "- `ADVr_TH`: Vertical Advective Flux of Potential Temperature (degC m^3/s)\n",
    "\n",
    "The xgcm `grid` object is then used to take the convergence of the horizontal heat advection.\n",
    "\n",
    "> **Note**: when using at least one recent version of `xgcm` (v0.8.1), errors were triggered when calling `diff_2d_vector`.\n",
    "> As an alternative, the `diff_2d_flux_llc90` function is included below.\n",
    "\n",
    "> **Note**: For the vertical fluxes `ADVr_TH`, `DFrE_TH`, and `DFrI_TH`, we need to make sure that sequence of dimensions are consistent.\n",
    "> When loading the fields use `.transpose('time','tile','k_l','j','i')`. Otherwise, the divergences will be not correct (at least for `tile = 12`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def da_replace_at_indices(da,indexing_dict,replace_values):\n",
    "    # replace values in xarray DataArray using locations specified by indexing_dict\n",
    "    array_data = da.data\n",
    "    indexing_dict_bynum = {}\n",
    "    for axis,dim in enumerate(da.dims):\n",
    "        if dim in indexing_dict.keys():\n",
    "            indexing_dict_bynum = {**indexing_dict_bynum,**{axis:indexing_dict[dim]}}\n",
    "    ndims = len(array_data.shape)\n",
    "    indexing_list = [':']*ndims\n",
    "    for axis in indexing_dict_bynum.keys():\n",
    "        indexing_list[axis] = indexing_dict_bynum[axis]\n",
    "    indexing_str = \",\".join(indexing_list)\n",
    "\n",
    "    # using exec isn't ideal, but this works for both NumPy and Dask arrays\n",
    "    exec('array_data['+indexing_str+'] = replace_values')        \n",
    "    \n",
    "    return da\n",
    "\n",
    "\n",
    "def diff_2d_flux_llc90(flux_vector_dict):\n",
    "    \"\"\"\n",
    "    A function that differences flux variables on the llc90 grid.\n",
    "    Can be used in place of xgcm's diff_2d_vector.\n",
    "    \"\"\"\n",
    "\n",
    "    u_flux = flux_vector_dict['X']\n",
    "    v_flux = flux_vector_dict['Y']\n",
    "    \n",
    "    u_flux_padded = u_flux.pad(pad_width={'i_g':(0,1)},mode='constant',constant_values=np.nan)\\\n",
    "                            .chunk({'i_g':u_flux.sizes['i_g']+1})\n",
    "    v_flux_padded = v_flux.pad(pad_width={'j_g':(0,1)},mode='constant',constant_values=np.nan)\\\n",
    "                            .chunk({'j_g':v_flux.sizes['j_g']+1})\n",
    "    \n",
    "    \n",
    "    # u flux padding\n",
    "    for tile in range(0,3):\n",
    "        u_flux_padded = da_replace_at_indices(u_flux_padded,{'tile':str(tile),'i_g':'-1'},\\\n",
    "                                              u_flux.isel(tile=tile+3,i_g=0).data)\n",
    "    for tile in range(3,6):\n",
    "        u_flux_padded = da_replace_at_indices(u_flux_padded,{'tile':str(tile),'i_g':'-1'},\\\n",
    "                                              v_flux.isel(tile=12-tile,j_g=0,i=slice(None,None,-1)).data)\n",
    "    u_flux_padded = da_replace_at_indices(u_flux_padded,{'tile':'6','i_g':'-1'},\\\n",
    "                                          u_flux.isel(tile=7,i_g=0).data)\n",
    "    for tile in range(7,9):\n",
    "        u_flux_padded = da_replace_at_indices(u_flux_padded,{'tile':str(tile),'i_g':'-1'},\\\n",
    "                                              u_flux.isel(tile=tile+1,i_g=0).data)\n",
    "    for tile in range(10,12):\n",
    "        u_flux_padded = da_replace_at_indices(u_flux_padded,{'tile':str(tile),'i_g':'-1'},\\\n",
    "                                              u_flux.isel(tile=tile+1,i_g=0).data)\n",
    "        \n",
    "    # v flux padding\n",
    "    for tile in range(0,2):\n",
    "        v_flux_padded = da_replace_at_indices(v_flux_padded,{'tile':str(tile),'j_g':'-1'},\\\n",
    "                                              v_flux.isel(tile=tile+1,j_g=0).data)\n",
    "    v_flux_padded = da_replace_at_indices(v_flux_padded,{'tile':'2','j_g':'-1'},\\\n",
    "                                          u_flux.isel(tile=6,j=slice(None,None,-1),i_g=0).data)\n",
    "    for tile in range(3,6):\n",
    "        v_flux_padded = da_replace_at_indices(v_flux_padded,{'tile':str(tile),'j_g':'-1'},\\\n",
    "                                              v_flux.isel(tile=tile+1,j_g=0).data)\n",
    "    v_flux_padded = da_replace_at_indices(v_flux_padded,{'tile':'6','j_g':'-1'},\\\n",
    "                                          u_flux.isel(tile=10,j=slice(None,None,-1),i_g=0).data)\n",
    "    for tile in range(7,10):\n",
    "        v_flux_padded = da_replace_at_indices(v_flux_padded,{'tile':str(tile),'j_g':'-1'},\\\n",
    "                                              v_flux.isel(tile=tile+3,j_g=0).data)\n",
    "    for tile in range(10,13):\n",
    "        v_flux_padded = da_replace_at_indices(v_flux_padded,{'tile':str(tile),'j_g':'-1'},\\\n",
    "                                              u_flux.isel(tile=12-tile,j=slice(None,None,-1),i_g=0).data)\n",
    "    \n",
    "    # take differences\n",
    "    diff_u_flux = u_flux_padded.diff('i_g')\n",
    "    diff_v_flux = v_flux_padded.diff('j_g')\n",
    "    \n",
    "    # include coordinates of input DataArrays and correct dimension/coordinate names\n",
    "    diff_u_flux = diff_u_flux.assign_coords(u_flux.coords).rename({'i_g':'i'})\n",
    "    diff_v_flux = diff_v_flux.assign_coords(v_flux.coords).rename({'j_g':'j'})\n",
    "\n",
    "    diff_flux_vector_dict = {'X':diff_u_flux,'Y':diff_v_flux}\n",
    "    \n",
    "    return diff_flux_vector_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **A note about memory usage**: Since the advection and diffusion calculations involve the full-depth ocean and are therefore memory intensive, we are going to be careful about limiting our memory usage at any given time: chunking our data in blocks that are sized based on our available memory, and clearing those blocks of data from our working memory space when we have finished computations with them. This is a little more complicated when using `Python` and `xarray` vs. some other computing languages, but here is the procedure we use:\n",
    "> - Close the *first* dataset where the data are loaded from source files\n",
    "> - Re-open the dataset so we clear any previously cached data (both the close and re-open seem to be necessary to clear the cache)\n",
    "> - Carry out computations\n",
    "> - Use `del` to delete any data variables where data was previously loaded using `compute`\n",
    "> - Close the first dataset and repeat the cycle on the next loop iteration\n",
    "\n",
    "> To make re-opening the dataset quicker, we will use a `pickle` object which saves the pointers created when calling `open_mfdataset` into memory. This is an important time-saver since we will be closing and re-opening this dataset a lot. For more background on why this procedure is being used, see the [Memory management in Python](https://ecco-v4-python-tutorial.readthedocs.io/Memory_management.html) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fluxes on land to zero (instead of NaN)\n",
    "ds['ADVx_TH'] = ds.ADVx_TH.where(ecco_grid.hFacW.values > 0,0)\n",
    "ds['ADVy_TH'] = ds.ADVy_TH.where(ecco_grid.hFacS.values > 0,0)\n",
    "ds['ADVr_TH'] = ds.ADVr_TH.where(ecco_grid.hFacC.values > 0,0)\n",
    "\n",
    "# transpose dimensions for xgcm (see note below)\n",
    "ds['ADVr_TH'] = ds.ADVr_TH.transpose('time','tile','k_l','j','i')\n",
    "\n",
    "# re-chunk arrays for better performance\n",
    "ds['ADVx_TH'] = ds['ADVx_TH'].chunk({'time':1,'k':-1,'tile':-1,'j':-1,'i_g':-1})\n",
    "ds['ADVy_TH'] = ds['ADVy_TH'].chunk({'time':1,'k':-1,'tile':-1,'j_g':-1,'i':-1})\n",
    "ds['ADVr_TH'] = ds['ADVr_TH'].chunk({'time':1,'tile':-1,'k_l':-1,'j':-1,'i':-1})\n",
    "\n",
    "\n",
    "# create pickled object with pointers to original flux files\n",
    "import pickle\n",
    "ecco_vars_int_pickled = pickle.dumps(ecco_vars_int)\n",
    "\n",
    "# close ecco_vars_int dataset\n",
    "ecco_vars_int.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: In case of the volume budget (and salinity conservation), the surface forcing (`oceFWflx`) is already included at the top level (`k_l = 0`) in ``WVELMASS``.  Thus, to keep the surface forcing term explicitly represented, one needs to zero out the values of ``WVELMASS`` at the surface so as to avoid double counting (see the [Volume budget closure](https://ecco-v4-python-tutorial.readthedocs.io/ECCO_v4_Volume_budget_closure.html) tutorial). This is not the case for the heat budget. `ADVr_TH` does not include the sea surface forcing. Thus, the vertical advective flux (at the air-sea interface) should not be zeroed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Original code to compute G_advection is commented below\n",
    "### (can use this if xgcm.diff_2d_vector is working properly\n",
    "### and memory constraints allow)\n",
    "\n",
    "\n",
    "# # compute horizontal components of flux divergence\n",
    "# ADVxy_diff = grid.diff_2d_vector({'X' : ds.ADVx_TH, 'Y' : ds.ADVy_TH}, boundary = 'fill')\n",
    "\n",
    "# # Convergence of horizontal advection (degC m^3/s)\n",
    "# adv_hConvH = (-(ADVxy_diff['X'] + ADVxy_diff['Y']))\n",
    "\n",
    "# # Convergence of vertical advection (degC m^3/s)\n",
    "# adv_vConvH = grid.diff(ADVr_TH, 'Z', boundary='fill')\n",
    "\n",
    "## Sum horizontal and vertical convergences and divide by volume (degC/s)\n",
    "# G_advection = (adv_hConvH + adv_vConvH)/vol\n",
    "\n",
    "\n",
    "### End of original code block\n",
    "\n",
    "\n",
    "def G_advection_compute(ds,ecco_vars_int_pickled,vol,time_isel=None,k_isel=None):\n",
    "    \"\"\"Computes advection tendency for given time and k indices (k indices must be continuous, without gaps)\"\"\"\n",
    "\n",
    "    if isinstance(time_isel,type(None)):\n",
    "        time_isel = np.arange(0,ds.sizes['time'])\n",
    "    if isinstance(k_isel,type(None)):\n",
    "        k_isel = np.arange(0,ds.sizes['k'])\n",
    "\n",
    "    if len(k_isel) > 1:\n",
    "        if (np.nanmin(np.diff(np.asarray(k_isel))) < 1) or (np.nanmax(np.diff(np.asarray(k_isel))) > 1):\n",
    "            raise ValueError('k_isel is not monotonically increasing or not continuous')        \n",
    "\n",
    "    \n",
    "    # re-open source dataset\n",
    "    ecco_vars_int = pickle.loads(ecco_vars_int_pickled)\n",
    "        \n",
    "    ## compute horizontal convergence\n",
    "\n",
    "    ADVx_TH = ds.ADVx_TH.isel(time=time_isel,k=k_isel).compute()\n",
    "    ADVy_TH = ds.ADVy_TH.isel(time=time_isel,k=k_isel).compute()\n",
    "    ADVxy_diff = diff_2d_flux_llc90({'X': ADVx_TH,\\\n",
    "                                     'Y': ADVy_TH})\n",
    "    \n",
    "    \n",
    "    # Convergence of horizontal advection (degC m^3/s)\n",
    "    adv_hConvH = (-(ADVxy_diff['X'] + ADVxy_diff['Y']))\n",
    "    \n",
    "    # transpose dimensions\n",
    "    adv_hConvH = adv_hConvH.transpose('time','tile','k','j','i')\n",
    "    \n",
    "    # restore time coordinate to DataArray if needed (can be lost in xgcm.diff_2d_vector operation)\n",
    "    adv_hConvH = adv_hConvH.assign_coords({'time':ds.time[time_isel].data}).compute()\n",
    "        \n",
    "    ## compute vertical convergence\n",
    "\n",
    "    if k_isel[-1] == ds.sizes['k']-1:\n",
    "        ADVr_TH = ds.ADVr_TH.isel(time=time_isel,k_l=k_isel).pad(pad_width={'k_l':(0,1)},mode='constant',constant_values=0).compute()        \n",
    "    else:\n",
    "        ADVr_TH = ds.ADVr_TH.isel(time=time_isel,k_l=np.append(k_isel,k_isel[-1]+1)).compute()\n",
    "    adv_vConvH = ADVr_TH.diff('k_l').rename({'k_l':'k'})\n",
    "    adv_vConvH = adv_vConvH.assign_coords({'k':ds.k[k_isel].data})\n",
    "        \n",
    "    # restore time coordinate to DataArray if needed (can be lost in xgcm.diff_2d_vector operation)\n",
    "    adv_vConvH = adv_vConvH.assign_coords({'time':ds.time[time_isel].data}).compute()\n",
    "        \n",
    "\n",
    "    ## Sum horizontal and vertical convergences and divide by volume (degC/s)\n",
    "    G_advection = ((adv_hConvH + adv_vConvH)/vol).compute()\n",
    "    \n",
    "    # delete the variables where data was actually loaded into memory\n",
    "    del ADVx_TH\n",
    "    del ADVy_TH\n",
    "    del adv_hConvH\n",
    "    del ADVr_TH\n",
    "    del adv_vConvH\n",
    "\n",
    "    # close the original dataset where the fluxes were loaded from the source files (needed to clear the data from cache)\n",
    "    ecco_vars_int.close()\n",
    "    \n",
    "    return G_advection\n",
    "\n",
    "\n",
    "\n",
    "def monthly_tmean_aggregate(function,ds,ecco_vars_int_pickled,vol,month_length_weights,time_chunksize=1,time_isel=None,k_isel=None):\n",
    "    \"\"\"Compute time mean by cumulatively summing array over time_isel indices, weighted by month length.\n",
    "       Includes variable time_chunksize to help us manage different memory environments;\n",
    "       larger chunks run faster but require more system memory.\"\"\"\n",
    "\n",
    "    if isinstance(time_isel,type(None)):\n",
    "        time_isel = np.arange(0,ds.sizes['time'])\n",
    "    \n",
    "    for time_chunk in range(int(np.ceil(len(time_isel)/time_chunksize))):\n",
    "        curr_time_isel = time_isel[(time_chunksize*time_chunk):np.fmin(time_chunksize*(time_chunk+1),len(time_isel))]\n",
    "        curr_array_computed = function(ds,ecco_vars_int_pickled,vol,time_isel=curr_time_isel,k_isel=k_isel)\n",
    "        if time_chunk == 0:\n",
    "            array_tmean = (month_length_weights.isel(time=curr_time_isel)*curr_array_computed).sum('time').compute()            \n",
    "        else:\n",
    "            array_tmean += (month_length_weights.isel(time=curr_time_isel)*curr_array_computed).sum('time').compute()            \n",
    "\n",
    "        del curr_array_computed        \n",
    "    \n",
    "    return array_tmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the time-mean $G^{\\theta}_\\textrm{advection}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_avail = psutil.virtual_memory().available\n",
    "print('Available memory:',mem_avail/(10**9),'GB')\n",
    "\n",
    "# chunk size to use when computing time mean with monthly_tmean_aggregate (not the same as dask chunksize)\n",
    "time_chunksize = int(np.round(mem_avail/(2**28)))\n",
    "time_chunksize = np.fmin(np.fmax(time_chunksize,1),ds.sizes['time'])\n",
    "print('Using time_chunksize =',time_chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "for idx, k in enumerate([0,1,25]):\n",
    "    p = ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC,\\\n",
    "                                      monthly_tmean_aggregate(G_advection_compute,\\\n",
    "                                                              ds,ecco_vars_int_pickled,vol,month_length_weights,\\\n",
    "                                                              time_chunksize=time_chunksize,k_isel=[k]),\\\n",
    "                                      show_colorbar=True,\n",
    "                                      cmin=-1e-6, cmax=1e-6, cmap='RdBu_r', user_lon_0=-67, dx=2, dy=2, \n",
    "                                      subplot_grid=[3,1,idx+1]);\n",
    "    p[1].set_title(r'$\\overline{G^\\theta_{advection}}$ at z = %i m (k = %i) [$^\\circ$C s$^{-1}$]'\\\n",
    "                   %(np.round(-ecco_grid.Z[k].values),k), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example $G^{\\theta}_\\textrm{advection}$ field at a particular time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_t_ind = 100\n",
    "tmp = str(ds.time.values[curr_t_ind])\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5));\n",
    "\n",
    "ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC,\\\n",
    "                              G_advection_compute(ds,ecco_vars_int_pickled,vol,time_isel=[curr_t_ind],k_isel=[0]),\\\n",
    "                              show_colorbar=True,\n",
    "                              cmin=-1e-6, cmax=1e-6, cmap='RdBu_r', user_lon_0=-67, dx=0.2, dy=0.2)\n",
    "plt.title(r'$G^\\theta_{advection}$ at the sea surface [$^\\circ$C s$^{-1}$] during ' + \n",
    "          str(tmp)[0:4] +'/' + str(tmp)[5:7], fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate tendency due to diffusive convergence ($G^{\\theta}_\\textrm{diffusion}$)\n",
    "\n",
    "The relevant fields from the diagnostic output here are\n",
    "- `DFxE_TH`: U Component Diffusive Flux of Potential Temperature (degC m^3/s)\n",
    "- `DFyE_TH`: V Component Diffusive Flux of Potential Temperature (degC m^3/s)\n",
    "- `DFrE_TH`: Vertical Diffusive Flux of Potential Temperature (Explicit part) (degC m^3/s)\n",
    "- `DFrI_TH`: Vertical Diffusive Flux of Potential Temperature (Implicit part) (degC m^3/s)\n",
    "> **Note**: Vertical diffusion has both an explicit (`DFrE_TH`) and an implicit (`DFrI_TH`) part.\n",
    "\n",
    "As with advective fluxes, we use the xgcm `grid` object to calculate the convergence of horizontal heat diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fluxes on land to zero (instead of NaN)\n",
    "ds['DFxE_TH'] = ds.DFxE_TH.where(ecco_grid.hFacW.values > 0,0)\n",
    "ds['DFyE_TH'] = ds.DFyE_TH.where(ecco_grid.hFacS.values > 0,0)\n",
    "ds['DFrE_TH'] = ds.DFrE_TH.where(ecco_grid.hFacC.values > 0,0)\n",
    "ds['DFrI_TH'] = ds.DFrI_TH.where(ecco_grid.hFacC.values > 0,0)\n",
    "\n",
    "# tranpose dimensions\n",
    "ds['DFrE_TH'] = ds.DFrE_TH.transpose('time','tile','k_l','j','i')\n",
    "ds['DFrI_TH'] = ds.DFrI_TH.transpose('time','tile','k_l','j','i')\n",
    "\n",
    "# re-chunk arrays for better performance\n",
    "ds['DFxE_TH'] = ds['DFxE_TH'].chunk({'time':12,'k':-1,'tile':-1,'j':-1,'i_g':-1})\n",
    "ds['DFyE_TH'] = ds['DFyE_TH'].chunk({'time':12,'k':-1,'tile':-1,'j_g':-1,'i':-1})\n",
    "ds['DFrE_TH'] = ds['DFrE_TH'].chunk({'time':12,'k_l':-1,'tile':-1,'j':-1,'i':-1})\n",
    "ds['DFrI_TH'] = ds['DFrI_TH'].chunk({'time':12,'k_l':-1,'tile':-1,'j':-1,'i':-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Original code to compute G_diffusion is commented below\n",
    "### (can use this if xgcm.diff_2d_vector is working properly\n",
    "### and memory constraints allow)\n",
    "\n",
    "\n",
    "# # compute horizontal components of flux divergence\n",
    "# DFxyE_diff = grid.diff_2d_vector({'X' : ds.DFxE_TH, 'Y' : ds.DFyE_TH}, boundary = 'fill')\n",
    "\n",
    "# # Convergence of horizontal diffusion (degC m^3/s)\n",
    "# dif_hConvH = (-(DFxyE_diff['X'] + DFxyE_diff['Y']))\n",
    "\n",
    "# # Convergence of vertical diffusion (degC m^3/s)\n",
    "# dif_vConvH = grid.diff(DFrE_TH + DFrI_TH, 'Z', boundary='fill')\n",
    "\n",
    "# # Sum horizontal and vertical convergences and divide by volume (degC/s)\n",
    "# G_diffusion = (dif_hConvH + dif_vConvH)/vol\n",
    "\n",
    "\n",
    "### End of original code block\n",
    "\n",
    "\n",
    "# complete horizontal divergence calculation, 12 time indices (1 year) at a time\n",
    "\n",
    "def G_diffusion_compute(ds,ecco_vars_int_pickled,vol,time_isel=None,k_isel=None):\n",
    "    \"\"\"Computes diffusion tendency for given time and k indices (k indices must be continuous, without gaps)\"\"\"\n",
    "\n",
    "    if isinstance(time_isel,type(None)):\n",
    "        time_isel = np.arange(0,ds.sizes['time'])\n",
    "    if isinstance(k_isel,type(None)):\n",
    "        k_isel = np.arange(0,ds.sizes['k'])\n",
    "\n",
    "    if len(k_isel) > 1:\n",
    "        if (np.nanmin(np.diff(np.asarray(k_isel))) < 1) or (np.nanmax(np.diff(np.asarray(k_isel))) > 1):\n",
    "            raise ValueError('k_isel is not monotonically increasing or not continuous')        \n",
    "\n",
    "    \n",
    "    # re-open source dataset\n",
    "    ecco_vars_int = pickle.loads(ecco_vars_int_pickled)\n",
    "    \n",
    "    ## compute horizontal convergence\n",
    "\n",
    "    DFxE_TH = ds.DFxE_TH.isel(time=time_isel,k=k_isel).compute()\n",
    "    DFyE_TH = ds.DFyE_TH.isel(time=time_isel,k=k_isel).compute()\n",
    "    DFxyE_diff = diff_2d_flux_llc90({'X': DFxE_TH,\\\n",
    "                                     'Y': DFyE_TH})\n",
    "    \n",
    "    # Convergence of horizontal advection (degC m^3/s)\n",
    "    dif_hConvH = (-(DFxyE_diff['X'] + DFxyE_diff['Y']))\n",
    "\n",
    "    # transpose dimensions\n",
    "    dif_hConvH = dif_hConvH.transpose('time','tile','k','j','i')\n",
    "    \n",
    "    # restore time coordinate to DataArray if needed (can be lost in xgcm.diff_2d_vector operation)\n",
    "    dif_hConvH = dif_hConvH.assign_coords({'time':ds.time[time_isel].data}).compute()\n",
    "    \n",
    "    ## compute vertical convergence\n",
    "    \n",
    "    if k_isel[-1] == ds.sizes['k']-1:\n",
    "        DFrE_TH = ds.DFrE_TH.isel(time=time_isel,k_l=k_isel).pad(pad_width={'k_l':(0,1)},mode='constant',constant_values=0).compute()\n",
    "        DFrI_TH = ds.DFrI_TH.isel(time=time_isel,k_l=k_isel).pad(pad_width={'k_l':(0,1)},mode='constant',constant_values=0).compute()        \n",
    "    else:\n",
    "        DFrE_TH = ds.DFrE_TH.isel(time=time_isel,k_l=np.append(k_isel,k_isel[-1]+1)).compute()\n",
    "        DFrI_TH = ds.DFrI_TH.isel(time=time_isel,k_l=np.append(k_isel,k_isel[-1]+1)).compute()        \n",
    "    dif_vConvH = (DFrE_TH + DFrI_TH).diff('k_l').rename({'k_l':'k'})\n",
    "    dif_vConvH = dif_vConvH.assign_coords({'k':ds.k[k_isel].data})\n",
    "    \n",
    "    # restore time coordinate to DataArray if needed (can be lost in xgcm.diff_2d_vector operation)\n",
    "    dif_vConvH = dif_vConvH.assign_coords({'time':ds.time[time_isel].data}).compute()\n",
    "        \n",
    "    ## Sum horizontal and vertical convergences and divide by volume (degC/s)\n",
    "    G_diffusion = ((dif_hConvH + dif_vConvH)/vol).compute()\n",
    "    \n",
    "    # delete the variables where data was actually loaded into memory\n",
    "    del DFxE_TH\n",
    "    del DFyE_TH\n",
    "    del dif_hConvH\n",
    "    del DFrE_TH\n",
    "    del DFrI_TH\n",
    "    del dif_vConvH\n",
    "\n",
    "    # close the original dataset where the fluxes were loaded from the source files (needed to clear the data from cache)\n",
    "    ecco_vars_int.close()\n",
    "    \n",
    "    return G_diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the time-mean $G^{\\theta}_\\textrm{diffusion}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_avail = psutil.virtual_memory().available\n",
    "print('Available memory:',mem_avail/(10**9),'GB')\n",
    "\n",
    "# chunk size to use when computing time mean with monthly_tmean_aggregate (not the same as dask chunksize)\n",
    "time_chunksize = int(np.round(mem_avail/(2**28)))\n",
    "time_chunksize = np.fmin(np.fmax(time_chunksize,1),ds.sizes['time'])\n",
    "print('Using time_chunksize =',time_chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "for idx, k in enumerate([0,1,25]):\n",
    "    p = ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC,\\\n",
    "                                      monthly_tmean_aggregate(G_diffusion_compute,\\\n",
    "                                                              ds,ecco_vars_int_pickled,vol,month_length_weights,\\\n",
    "                                                              time_chunksize=time_chunksize,k_isel=[k]),\\\n",
    "                                      show_colorbar=True,\n",
    "                                      cmin=-3e-6, cmax=3e-6, cmap='RdBu_r', user_lon_0=-67, dx=2, dy=2, \n",
    "                                      subplot_grid=[3,1,idx+1]);\n",
    "    p[1].set_title(r'$\\overline{G^\\theta_{diffusion}}$ at z = %i m (k = %i) [$^\\circ$C s$^{-1}$]'\\\n",
    "                   %(np.round(-ecco_grid.Z[k].values),k), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example $G^{\\theta}_\\textrm{diffusion}$ field at a particular time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_t_ind = 100\n",
    "tmp = str(ds.time.values[curr_t_ind])\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5));\n",
    "\n",
    "ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC,\\\n",
    "                              G_diffusion_compute(ds,ecco_vars_int_pickled,vol,time_isel=[curr_t_ind],k_isel=[0]),\\\n",
    "                              show_colorbar=True,\n",
    "                              cmin=-3e-6, cmax=3e-6, cmap='RdBu_r', user_lon_0=-67, dx=0.2, dy=0.2)\n",
    "plt.title(r'$G^\\theta_{diffusion}$ at the sea surface [$^\\circ$C s$^{-1}$] during ' + \n",
    "          str(tmp)[0:4] +'/' + str(tmp)[5:7], fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate tendency due to forcing ($G^{\\theta}_\\textrm{forcing}$)\n",
    "Finally, we evaluate the local forcing term due to surface heat and geothermal fluxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surface heat flux\n",
    "For the surface contribution, there are two relevant model diagnostics:\n",
    "- `TFLUX`: total heat flux (match heat-content variations) (W/m^2)\n",
    "- `oceQsw`: net Short-Wave radiation (+=down) (W/m^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining terms needed for evaluating surface heat forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = ecco_grid.Z.compute()\n",
    "RF = np.concatenate([ecco_grid.Zp1.values[:-1],[np.nan]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**: `Z` and `Zp1` are used in deriving surface heat penetration. MATLAB code uses `RF` from `mygrid` structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = R*np.exp(1.0/zeta1*RF[:-1]) + (1.0-R)*np.exp(1.0/zeta2*RF[:-1])\n",
    "q2 = R*np.exp(1.0/zeta1*RF[1:]) + (1.0-R)*np.exp(1.0/zeta2*RF[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction for the 200m cutoff\n",
    "zCut = np.where(Z < -200)[0][0]\n",
    "q1[zCut:] = 0\n",
    "q2[zCut-1:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create xarray data arrays\n",
    "q1 = xr.DataArray(q1,coords=[Z.k],dims=['k'])\n",
    "q2 = xr.DataArray(q2,coords=[Z.k],dims=['k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute vertically penetrating flux\n",
    "Given the penetrating nature of the shortwave term, to properly evaluate the local forcing term, `oceQsw` must be removed from `TFLUX` (which contains the net latent, sensible, longwave, and shortwave contributions) and redistributed vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Land masks\n",
    "# Make copy of hFacC\n",
    "mskC = ecco_grid.hFacC.copy(deep=True).compute()\n",
    "\n",
    "# Change all fractions (ocean) to 1. land = 0\n",
    "mskC.values[mskC.values>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortwave flux below the surface (W/m^2)\n",
    "forcH_subsurf = ((q1*(mskC==1)-q2*(mskC.shift(k=-1)==1))*ds.oceQsw).transpose('time','tile','k','j','i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface heat flux (W/m^2)\n",
    "forcH_surf = ((ds.TFLUX - (1-(q1[0]-q2[0]))*ds.oceQsw)\\\n",
    "              *mskC[0]).transpose('time','tile','j','i').assign_coords(k=0).expand_dims('k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-depth sea surface forcing (W/m^2)\n",
    "forcH = xr.concat([forcH_surf,forcH_subsurf[:,:,1:]], dim='k').transpose('time','tile','k','j','i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geothermal flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geothermal flux contribution is not accounted for in any of the standard model diagnostics provided as output. Rather, this term, which is time invariant, is provided in the input file `geothermalFlux.bin` contained in the ancillary data archive. This archive can be downloaded from [PO.DAAC](https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_ANCILLARY_DATA_V4R4/ancillary_data_input_forcing_ECCO_V4r4.tar.gz), though accessing it there requires downloading a much larger tarball of files (~192 GB). So, for the time being, the `geothermalFlux.bin` file is also stored on the tutorial Github and can be downloaded [here](https://github.com/ECCO-GROUP/ECCO-v4-Python-Tutorial/raw/master/misc/geothermalFlux.bin).\n",
    "\n",
    "> **Note**: The code cell below assumes `geothermalFlux.bin` has been placed in `~/Downloads`, or is in the cloned GitHub repository under `~/ECCO-v4-Python-Tutorial/misc` or `~/git_repos/ECCO-v4-Python-Tutorial/misc`. Change the directory `geoflx_dir` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geothermal heat flux using the routine 'read_llc_to_tiles'.\n",
    "geoflx_filename = 'geothermalFlux.bin'\n",
    "if exists(join(user_home_dir,'Downloads',geoflx_filename)):\n",
    "    geoflx_dir = join(user_home_dir,'Downloads')\n",
    "elif exists(join(user_home_dir,'ECCO-v4-Python-Tutorial','misc',geoflx_filename)):\n",
    "    geoflx_dir = join(user_home_dir,'ECCO-v4-Python-Tutorial','misc')\n",
    "elif exists(join(user_home_dir,'git_repos','ECCO-v4-Python-Tutorial','misc',geoflx_filename)):\n",
    "    geoflx_dir = join(user_home_dir,'git_repos','ECCO-v4-Python-Tutorial','misc')\n",
    "else:\n",
    "    raise FilePathError('Can not locate '+geoflx_filename+' in default directories.\\n'\\\n",
    "                        +'Please specify geoflx_dir.')\n",
    "# geoflx_dir = join(user_home_dir,'Downloads')\n",
    "geoflx = ecco.read_llc_to_tiles(geoflx_dir, 'geothermalFlux.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geothermal flux dataset needs to be saved as an xarray data array with the same format as the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy array to an xarray DataArray with matching dimensions as the monthly mean fields\n",
    "geoflx_llc = xr.DataArray(geoflx,coords={'tile': ecco_monthly_mean.tile.values,\n",
    "                                         'j': ecco_monthly_mean.j.values,\n",
    "                                         'i': ecco_monthly_mean.i.values},dims=['tile','j','i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5));\n",
    "\n",
    "ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC, geoflx_llc,show_colorbar=True,cmap='magma', \n",
    "                              user_lon_0=-67, dx=0.2, dy=0.2)\n",
    "plt.title(r'Geothermal heat flux [W m$^{-2}$]', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geothermal flux needs to be a three dimensional field since the sources are distributed along the ocean floor at various depths. This requires a three dimensional mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3d bathymetry mask\n",
    "mskC_shifted = mskC.shift(k=-1)\n",
    "\n",
    "mskC_shifted.values[-1,:,:,:] = 0\n",
    "mskb = mskC - mskC_shifted\n",
    "\n",
    "# Create 3d field of geothermal heat flux\n",
    "geoflx3d = geoflx_llc * mskb.transpose('k','tile','j','i')\n",
    "GEOFLX = geoflx3d.transpose('k','tile','j','i')\n",
    "GEOFLX.attrs = {'standard_name': 'GEOFLX','long_name': 'Geothermal heat flux','units': 'W/m^2'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total forcing ($G^{\\theta}_\\textrm{forcing}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add geothermal heat flux to forcing field and convert from W/m^2 to degC/s\n",
    "G_forcing = ((forcH + GEOFLX)/(rhoconst*c_p))/(ecco_grid.hFacC*ecco_grid.drF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the time-mean $G^{\\theta}_\\textrm{forcing}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_forcing_mean = (G_forcing*month_length_weights).sum('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "for idx, k in enumerate([0,1,25]):\n",
    "    p = ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC, G_forcing_mean.isel(k=k),show_colorbar=True,\n",
    "                                      cmin=-3e-6, cmax=3e-6, cmap='RdBu_r', user_lon_0=-67, dx=2, dy=2, \n",
    "                                      subplot_grid=[3,1,idx+1]);\n",
    "    p[1].set_title(r'$\\overline{G^\\theta_{forcing}}$ at z = %i m (k = %i) [$^\\circ$C s$^{-1}$]'\\\n",
    "                   %(np.round(-ecco_grid.Z[k].values),k), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\overline{G^\\theta_{forcing}}$ is focused at the sea surface and much smaller (essentially zero) at depth. $\\overline{G^\\theta_{forcing}}$ is negative for most of the ocean (away from the equator). The spatial pattern in the surface forcing is the same as for diffusion but with opposite sign (see maps for $\\overline{G^\\theta_{diffusion}}$ above). This makes sense as forcing is to a large extent balanced by diffusion within the mixed layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example $G^{\\theta}_\\textrm{forcing}$ field at a particular time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_t_ind = 100\n",
    "tmp = str(G_forcing.time.values[curr_t_ind])\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5));\n",
    "\n",
    "ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC, G_forcing.isel(time=curr_t_ind,k=0),show_colorbar=True,\n",
    "                              cmin=-5e-6, cmax=5e-6, cmap='RdBu_r', user_lon_0=-67, dx=0.2, dy=0.2)\n",
    "plt.title(r'$G^\\theta_{forcing}$ at the sea surface [$^\\circ$C s$^{-1}$] during ' + \n",
    "          str(tmp)[0:4] +'/' + str(tmp)[5:7], fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to dataset\n",
    "Now that we have all the terms evaluated, let's save them to a dataset. Here are two examples:\n",
    "- Zarr is a new format that is used for cloud storage.\n",
    "- Netcdf is the more traditional format that most people are familiar with.\n",
    "\n",
    "When saving this heat budget dataset, the zarr file is ~15 GB, while the NetCDF file is ~53 GB. So zarr can be more efficient for storage.\n",
    "\n",
    "### Add all variables to a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = ['G_total','G_advection','G_diffusion','G_forcing']\n",
    "\n",
    "G_total = G_total.transpose('time','tile','k','j','i')\n",
    "\n",
    "adv_diff_written = True\n",
    "\n",
    "ds_budg = xr.Dataset(data_vars={})\n",
    "for varname in varnames:\n",
    "    if varname not in globals():\n",
    "        # create empty dask arrays for G_advection and G_diffusion (to be written later)\n",
    "        import dask.array as da\n",
    "        ds_budg[varname] = (['time','tile','k','j','i'],\\\n",
    "                            da.empty((ds.sizes['time'],13,50,90,90),dtype='float32',\\\n",
    "                                    chunks=(1,13,50,90,90)))\n",
    "        adv_diff_written = False\n",
    "    else:\n",
    "        ds_budg[varname] = globals()[varname].chunk(chunks={'time':1,'tile':13,'k':50,'j':90,'i':90})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add surface forcing (degC/s)\n",
    "ds_budg['Qnet'] = ((forcH /(rhoconst*c_p))\\\n",
    "                  /(ecco_grid.hFacC*ecco_grid.drF)).chunk(chunks={'time':1,'tile':13,'k':50,'j':90,'i':90})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add shortwave penetrative flux (degC/s)\n",
    "#Since we only are interested in the subsurface heat flux we need to zero out the top cell\n",
    "SWpen = ((forcH_subsurf /(rhoconst*c_p))/(ecco_grid.hFacC*ecco_grid.drF)).where(forcH_subsurf.k>0).fillna(0.)\n",
    "ds_budg['SWpen'] = SWpen.where(ecco_grid.hFacC>0).chunk(chunks={'time':1,'tile':13,'k':50,'j':90,'i':90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: `Qnet` and `SWpen` are included in `G_forcing` and are not necessary to close the heat budget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_budg.time.encoding = {}\n",
    "ds_budg = ds_budg.reset_coords(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir is set to ~/Downloads below;\n",
    "# change if you want to save somewhere else\n",
    "save_dir = join(user_home_dir,'Downloads')\n",
    "\n",
    "# first query how much storage is free\n",
    "# the zarr file will occupy ~15 GB, so require 20 GB free storage as a buffer\n",
    "\n",
    "import shutil\n",
    "free_storage = shutil.disk_usage(save_dir).free\n",
    "print(f'Free storage: {free_storage/(10**9)} GB')\n",
    "\n",
    "# query how much memory is available\n",
    "# (influences how this large archive will be computed and stored)\n",
    "mem_avail = psutil.virtual_memory().available\n",
    "print('Available memory:',mem_avail/(10**9),'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb on\n",
    "\n",
    "### Original code to save dataset to zarr archive\n",
    "\n",
    "# zarr_save_location = join(save_dir,'eccov4r4_budg_heat')\n",
    "# ds.to_zarr(zarr_save_location)\n",
    "\n",
    "### End of original code block\n",
    "\n",
    "\n",
    "def zarr_archive_tloop(function,save_location,varname,\\\n",
    "                       ds,ecco_vars_int_pickled,vol,time_chunksize=1,time_isel=None,k_isel=None):\n",
    "    \"\"\"\n",
    "    Compute array using function and save to zarr archive, \n",
    "    by looping through time chunks of size time_chunksize.\n",
    "    This has cleaner memory usage than just relying on dask chunking.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(time_isel,type(None)):\n",
    "        time_isel = np.arange(0,ds.sizes['time'])\n",
    "    \n",
    "    for time_chunk in range(int(np.ceil(len(time_isel)/time_chunksize))):\n",
    "        if exists(save_location):\n",
    "            ds_to_write = xr.open_zarr(save_location)\n",
    "        curr_time_isel = time_isel[(time_chunksize*time_chunk):np.fmin(time_chunksize*(time_chunk+1),len(time_isel))]\n",
    "        ds_to_write[varname] = da_replace_at_indices(ds_to_write[varname],\\\n",
    "                                                    {'time':str(curr_time_isel[0])+':'+str(curr_time_isel[-1]+1)},\\\n",
    "                                                    function(ds,ecco_vars_int_pickled,vol,time_isel=curr_time_isel,k_isel=k_isel))\n",
    "        ds_to_write[varname].to_dataset().to_zarr(save_location,mode=\"a\")\n",
    "        ds_to_write.close()\n",
    "\n",
    "\n",
    "# the zarr archive will occupy ~15 GB, so require 20 GB free storage as a buffer\n",
    "zarr_save_location = join(save_dir,'eccov4r4_budg_heat')\n",
    "if free_storage >= 20*(10**9):\n",
    "    # chunk size to use when computing G_advection and G_diffusion (not the same as dask chunksize)\n",
    "    time_chunksize = int(np.round(mem_avail/(10**9)))\n",
    "    # time_chunksize = ds.sizes['time']\n",
    "    time_chunksize = np.fmin(np.fmax(time_chunksize,1),ds.sizes['time'])\n",
    "    print('Using time_chunksize =',time_chunksize)\n",
    "    if mem_avail >= 20*(10**9):\n",
    "        if not adv_diff_written:\n",
    "            ds_budg['G_advection'] = G_advection_compute(ds,ecco_vars_int_pickled,vol)\n",
    "            ds_budg['G_diffusion'] = G_diffusion_compute(ds,ecco_vars_int_pickled,vol)\n",
    "        with ProgressBar():\n",
    "            ds_budg.to_zarr(zarr_save_location)\n",
    "    else:\n",
    "        ecco_vars_int.close()\n",
    "        for varname in ds_budg.data_vars:\n",
    "            ds_budg[varname].to_dataset().to_zarr(zarr_save_location,mode=\"a\")                \n",
    "            if varname in ['G_advection','G_diffusion']:                \n",
    "                zarr_archive_tloop(eval(varname+'_compute'),zarr_save_location,varname,\\\n",
    "                                   ds,ecco_vars_int_pickled,vol,time_chunksize=time_chunksize)\n",
    "else:\n",
    "    print('Insufficient storage to save global budget terms to disk as zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save budget as netcdf, set save_netcdf = True\n",
    "save_netcdf = False\n",
    "\n",
    "# the netcdf file will occupy ~53 GB, so require 60 GB free storage as a buffer\n",
    "if save_netcdf:\n",
    "    if free_storage >= 60*(10**9):\n",
    "        with ProgressBar():\n",
    "            ds.to_netcdf(join(save_dir,'eccov4r4_budg_heat.nc'), format='NETCDF4')\n",
    "    else:\n",
    "        print('Insufficient storage to save global budget terms to disk as netcdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load budget variables from file\n",
    "After having saved the budget terms to file, we can load the dataset like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load terms from zarr dataset\n",
    "G_budget = xr.open_zarr(join(save_dir,'eccov4r4_budg_heat'))\n",
    "G_total = G_budget.G_total\n",
    "G_advection = G_budget.G_advection\n",
    "G_diffusion = G_budget.G_diffusion\n",
    "G_forcing = G_budget.G_forcing\n",
    "Qnet = G_budget.Qnet\n",
    "SWpen = G_budget.SWpen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you saved it as a netcdf file:\n",
    "``` python\n",
    "# Load terms from netcdf file\n",
    "G_budget = xr.open_mfdataset(join(save_dir,'eccov4r4_budg_heat.nc'))\n",
    "G_total_tendency = G_budget.G_total_tendency\n",
    "G_advection = G_budget.G_advection\n",
    "G_diffusion = G_budget.G_diffusion\n",
    "G_forcing = G_budget.G_forcing\n",
    "Qnet = G_budget.Qnet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between LHS and RHS of the budget equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total convergence\n",
    "ConvH = G_advection + G_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of terms in RHS of equation\n",
    "rhs = ConvH + G_forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ((rhs-G_total).sum(dim='k')*month_length_weights).sum(dim='time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC, res,\n",
    "                              cmin=-5e-12, cmax=5e-12, show_colorbar=True, cmap='RdBu_r',dx=0.2, dy=0.2)\n",
    "plt.title(r'Residual $\\partial \\theta / \\partial t$ [$^\\circ$C s$^{-1}$]: RHS - LHS', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual (summed over depth and time) is essentially zero everywhere. What if we omit the geothermal heat flux?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual when omitting geothermal heat flux\n",
    "res_geo = ((ConvH + Qnet - G_total).sum(dim='k')*month_length_weights).sum(dim='time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC, res_geo,\n",
    "                              cmin=-1e-9, cmax=1e-9, show_colorbar=True, cmap='RdBu_r', dx=0.2, dy=0.2)\n",
    "plt.title(r'Residual due to omitting geothermal heat [$^\\circ$C s$^{-1}$] ', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the contribution from geothermal flux in the heat budget is well above the residual (by *three orders of magnitude*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual when omitting shortwave penetrative heat flux\n",
    "res_sw = (rhs-SWpen-G_total).sum(dim='k').sum(dim='time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "ecco.plot_proj_to_latlon_grid(ecco_grid.XC, ecco_grid.YC, res_sw,\n",
    "                              cmin=-5e-4, cmax=5e-4, show_colorbar=True, cmap='RdBu_r', dx=0.2, dy=0.2)\n",
    "plt.title(r'Residual due to omitting shortwave penetrative heat flux [$^\\circ$C s$^{-1}$] ', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of subsurface heat fluxes, shortwave penetration represents a much larger heat flux compared to geothermal heat flux (by around *three orders of magnitude*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of residuals\n",
    "We can look at the distribution of residuals to get a little more confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.abs(res).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3));\n",
    "\n",
    "plt.hist(tmp[tmp > 0],np.linspace(0, 2.e-12, 200));\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing residuals vertically and temporally yields < $10^{-12}$ $^\\circ$C s$^{-1}$ for most grid points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat budget closure through time\n",
    "\n",
    "### Global average budget closure\n",
    "\n",
    "Another way of demonstrating heat budget closure is to show the global spatially-averaged `THETA` tendency terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute volume (m^3) if not already computed\n",
    "if 'vol' not in locals():\n",
    "    vol = (ecco_grid.rA*ecco_grid.drF*ecco_grid.hFacC).transpose('tile','k','j','i').compute()\n",
    "elif not isinstance(vol,np.ndarray):\n",
    "    vol = (ecco_grid.rA*ecco_grid.drF*ecco_grid.hFacC).transpose('tile','k','j','i').compute()\n",
    "\n",
    "# Take volume-weighted mean of these terms\n",
    "tmp_a=(G_total*vol).sum(dim=('k','i','j','tile')).compute()/vol.sum()\n",
    "tmp_b=(G_advection*vol).sum(dim=('k','i','j','tile')).compute()/vol.sum()\n",
    "tmp_c=(G_diffusion*vol).sum(dim=('k','i','j','tile')).compute()/vol.sum()\n",
    "tmp_d=(G_forcing*vol).sum(dim=('k','i','j','tile')).compute()/vol.sum()\n",
    "\n",
    "# tmp_e=(rhs*vol).sum(dim=('k','i','j','tile')).compute()/vol.sum()\n",
    "# # save time by not re-computing G_advection, G_diffusion, G_forcing to compute rhs\n",
    "tmp_e = tmp_b + tmp_c + tmp_d\n",
    "\n",
    "# Result is five time series\n",
    "tmp_a.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14,8))\n",
    "\n",
    "plt.sca(axs[0,0])\n",
    "tmp_a.plot(color='k',lw=2)\n",
    "tmp_e.plot(color='grey')\n",
    "axs[0,0].set_title(r'a. $G^\\theta_{total}$ (black) / RHS (grey) [$^\\circ$C s$^{-1}$]', fontsize=12)\n",
    "plt.grid()\n",
    "\n",
    "plt.sca(axs[0,1])\n",
    "tmp_b.plot(color='r')\n",
    "axs[0,1].set_title(r'b. $G^\\theta_{advection}$ [$^\\circ$C s$^{-1}$]', fontsize=12)\n",
    "plt.grid()\n",
    "\n",
    "plt.sca(axs[1,0])\n",
    "tmp_c.plot(color='orange')\n",
    "axs[1,0].set_title(r'c. $G^\\theta_{diffusion}$ [$^\\circ$C s$^{-1}$]', fontsize=12)\n",
    "plt.grid()\n",
    "\n",
    "plt.sca(axs[1,1])\n",
    "tmp_d.plot(color='b')\n",
    "axs[1,1].set_title(r'd. $G^\\theta_{forcing}$ [$^\\circ$C s$^{-1}$]', fontsize=12)\n",
    "plt.grid()\n",
    "plt.subplots_adjust(hspace = .5, wspace=.2)\n",
    "plt.suptitle('Global Heat Budget', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When averaged over the entire ocean the ocean heat transport terms ($G^\\theta_\\textrm{advection}$ and $G^\\theta_\\textrm{diffusion}$)  have no net impact on $G^\\theta_\\textrm{total}$ (i.e., $\\partial \\theta / \\partial t$).  This makes sense because $G^\\theta_\\textrm{advection}$ and $G^\\theta_\\textrm{diffusion}$ can only redistributes heat. Globally, $\\theta$ can only change via $G^\\theta_\\textrm{forcing}$.\n",
    "\n",
    "### Local heat budget closure\n",
    "\n",
    "Locally we expect that heat divergence can impact $\\theta$. This is demonstrated for a single grid point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick any set of indices (tile, k, j, i) corresponding to an ocean grid point\n",
    "t,k,j,i = (6,10,40,29)\n",
    "print(t,k,j,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_a = G_total.isel(tile=t,k=k,j=j,i=i).compute()\n",
    "tmp_b = G_advection.isel(tile=t,k=k,j=j,i=i).compute()\n",
    "tmp_c = G_diffusion.isel(tile=t,k=k,j=j,i=i).compute()\n",
    "tmp_d = G_forcing.isel(tile=t,k=k,j=j,i=i).compute()\n",
    "# tmp_e = rhs.isel(tile=t,k=k,j=j,i=i)\n",
    "\n",
    "# # save time by not re-computing G_advection, G_diffusion, G_forcing to compute rhs\n",
    "tmp_e = tmp_b + tmp_c + tmp_d\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14,8))\n",
    "\n",
    "plt.sca(axs[0,0])\n",
    "tmp_a.plot(color='k',lw=2)\n",
    "tmp_e.plot(color='grey')\n",
    "axs[0,0].set_title(r'a. $G^\\theta_{total}$ (black) / RHS (grey) [$^\\circ$C s$^{-1}$]', fontsize=12)\n",
    "plt.grid()\n",
    "\n",
    "plt.sca(axs[0,1])\n",
    "tmp_b.plot(color='r')\n",
    "axs[0,1].set_title(r'b. $G^\\theta_{advection}$ [$^\\circ$C s$^{-1}$]', fontsize=12)\n",
    "plt.grid()\n",
    "\n",
    "plt.sca(axs[1,0])\n",
    "tmp_c.plot(color='orange')\n",
    "axs[1,0].set_title(r'c. $G^\\theta_{diffusion}$ [$^\\circ$C s$^{-1}$]', fontsize=12)\n",
    "plt.grid()\n",
    "\n",
    "plt.sca(axs[1,1])\n",
    "tmp_d.plot(color='b')\n",
    "axs[1,1].set_title(r'd. $G^\\theta_{forcing}$ [$^\\circ$C s$^{-1}$]', fontsize=12)\n",
    "plt.grid()\n",
    "plt.subplots_adjust(hspace = .5, wspace=.2)\n",
    "plt.suptitle('Heat Budget for one grid point (tile = %i, k = %i, j = %i, i = %i)'%(t,k,j,i), fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the heat divergence terms do contribute to $\\theta$ variations at a single point. Local heat budget closure is also confirmed at this grid point as we see that the sum of terms on the RHS (grey line) equals the LHS (black line).\n",
    "\n",
    "For the Arctic grid point, there is a clear seasonal cycles in both $G^\\theta_\\textrm{total}$, $G^\\theta_\\textrm{diffusion}$ and $G^\\theta_\\textrm{forcing}$. The seasonal cycle in $G^\\theta_\\textrm{forcing}$ seems to be the reverse of $G^\\theta_\\textrm{total}$ and $G^\\theta_\\textrm{diffusion}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6));\n",
    "tmp_a.groupby('time.month').mean('time').plot(color='k',lw=3)\n",
    "tmp_b.groupby('time.month').mean('time').plot(color='r')\n",
    "tmp_c.groupby('time.month').mean('time').plot(color='orange')\n",
    "tmp_d.groupby('time.month').mean('time').plot(color='b')\n",
    "tmp_e.groupby('time.month').mean('time').plot(color='grey')\n",
    "plt.ylabel(r'$\\partial\\theta$/$\\partial t$ [$^\\circ$C s$^{-1}$]', fontsize=12)\n",
    "plt.grid()\n",
    "plt.title('Climatological seasonal cycles', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean seasonal cycle of the total is a balance between advection and diffusion. However, this is likely depth-dependent. How does the balance look across the upper 200 meter at that location?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-mean vertical profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_aa=G_total.isel(tile=t,j=j,i=i).mean('time').compute()\n",
    "tmp_bb=G_advection.isel(tile=t,j=j,i=i).mean('time').compute()\n",
    "tmp_cc=G_diffusion.isel(tile=t,j=j,i=i).mean('time').compute()\n",
    "tmp_dd=G_forcing.isel(tile=t,j=j,i=i).mean('time').compute()\n",
    "# tmp_ee=rhs.isel(tile=t,j=j,i=i).mean('time').compute()\n",
    "tmp_ee = tmp_bb + tmp_cc + tmp_dd\n",
    "\n",
    "\n",
    "fig = plt.subplots(1, 2, sharey=True, figsize=(12,7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(tmp_aa, -ecco_grid.Z,\n",
    "         lw=4, color='black', marker='.', label=r'$G^\\theta_{total}$ (LHS)')\n",
    "\n",
    "plt.plot(tmp_bb, -ecco_grid.Z, \n",
    "         lw=2, color='red', marker='.', label=r'$G^\\theta_{advection}$')\n",
    "\n",
    "plt.plot(tmp_cc, -ecco_grid.Z, \n",
    "         lw=2, color='orange', marker='.', label=r'$G^\\theta_{diffusion}$')\n",
    "\n",
    "plt.plot(tmp_dd, -ecco_grid.Z, \n",
    "         lw=2, color='blue', marker='.', label=r'$G^\\theta_{forcing}$')\n",
    "plt.plot(tmp_ee, ecco_grid.Z, lw=1, color='grey', marker='.', label='RHS')\n",
    "plt.xlabel(r'$\\partial\\theta$/$\\partial t$ [$^\\circ$C s$^{-1}$]', fontsize=14)\n",
    "plt.ylim([200,0])\n",
    "plt.ylabel('Depth (m)', fontsize=14)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.legend(loc='lower left', frameon=False, fontsize=12)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(tmp_aa, -ecco_grid.Z,\n",
    "         lw=4, color='black', marker='.', label=r'$G^\\theta_{total}$ (LHS)')\n",
    "plt.plot(tmp_ee, -ecco_grid.Z, lw=1, color='grey', marker='.', label='RHS')\n",
    "plt.xlabel(r'$\\partial\\theta$/$\\partial t$ [$^\\circ$C s$^{-1}$]', fontsize=14)\n",
    "plt.ylim([200,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance between surface forcing and diffusion in the top layers. Balance between advection and diffusion at depth."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
